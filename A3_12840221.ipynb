{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A3_12840221.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JennyZhan165/ML_A3_12840221/blob/master/A3_12840221.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-I6VNgJjZnx6",
        "colab_type": "text"
      },
      "source": [
        "### Assignment 3: Take Home Exam\n",
        "\n",
        "Yunhan Zhan 12840221"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSye_v9EPunT",
        "colab_type": "text"
      },
      "source": [
        "### Question 4 \n",
        "\n",
        "One of the themes in the machine learning models we've looked at this semester is large numbers of parameters that are changed by tiny amounts. Why do so many apparently different models use such similar techniques? Are there other ways to approach the problem of learning? Are there also commonalities in the way the amounts to be changed are determined? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxLLla20QAjI",
        "colab_type": "text"
      },
      "source": [
        "###1. Introduction\n",
        "\n",
        "There are various machine learning algorithms can be applied for different problems now (Refer to Figure 1). A machine Learning model is defined as a mathematical model which can predict and classify the data and involves a large number of parameters. Therefore, machine learning models are parameterized (Sayak 2018), and even small changes about the parameter will also affect the classification and prediction results. Therefore, it is very important to adjust the parameters because the best combination of the parameters can enhance the performance of the machine learning model and forecast accuracy in the results. When we are designing a model, we’d like to explore as many possibilities as we can by changing the sets of parameters. The process of finding the best set of parameters called tuning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsSVSUr0T5kv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "outputId": "e1817f04-eebc-4c2b-bbc5-584ed4d940c9"
      },
      "source": [
        "from IPython.display import display, HTML\n",
        "display(HTML('''<img src='https://github.com//JennyZhan165/ML_A3_12840221/blob/master/ml%20models.png?raw=true' width='800'>'''))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src='https://github.com//JennyZhan165/ML_A3_12840221/blob/master/ml%20models.png?raw=true' width='800'>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMs_vIKnQE9y",
        "colab_type": "text"
      },
      "source": [
        "###2. Parameter\n",
        "\n",
        "As we can see, parameter plays an important role in machine learning model. There are two types of model can be discussed in machine learning model. The first one is called ‘Model Parameter’ and the another one is ‘Hyperparameter’. Model parameter is defined as transferring the input data to the output. Additionally, hyperparameter is defined as how the model is structured (Jason 2014). The main difference between these two types of parameter is that the model parameters can gathered from the training data, while hyperparameters cannot. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsZs7VeNRYRi",
        "colab_type": "text"
      },
      "source": [
        "####2.1 Model Parameter\n",
        "\n",
        "In terms of model parameter, Aloïs mentioned a model parameter is the configuration variable that is internal to the model and the values can be estimated from the dataset. For instance, in terms of a Support Vector Machine (SVM), the support vector is the model parameter, split points in Decision Tree and the coefficient for Linear Regression is also a model parameter.\n",
        "\n",
        "####2.2 Hyperparameter\n",
        "\n",
        "There is another parameter named hyperparameter which is a configuration that is external to the model and the value is always be fixed before the data processing begin. The benefits of identifying the good hyperparameters are search the spaces of hyperparameters efficiently and easily manage large set of hyperparameter tuning (Prahbu 2018). There are many hyperparameters in machine learning algorithm, as shown in the Figure below. For Support Vector Machine, the C and sigma are the hyperparameter. Additionally, the K in K-Nearest Neighbors (KNN) is also hyperparameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qj4_MaQzWb9T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "outputId": "db2ee4de-f24d-4f23-f173-5f8a0d6802e3"
      },
      "source": [
        "display(HTML('''<img src='https://github.com//JennyZhan165/ML_A3_12840221/blob/master/Figure2.png?raw=true' width='600'>'''))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src='https://github.com//JennyZhan165/ML_A3_12840221/blob/master/Figure2.png?raw=true' width='600'>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LT0T0x3rQILb",
        "colab_type": "text"
      },
      "source": [
        "### 3. Hyperparameter tuning algorithm\n",
        "\n",
        "Algorithm tuning usually the last step before implementing the results. Finding a good set of parameters is a pretty difficult task. Therefore, there are plenty of ideas to optimize the parameters. Few algorithms that I discussed below are based on this idea. Grid search and random search are the first option for optimization because they are easy to understand and code in any programming language."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "046QWzToRt_e",
        "colab_type": "text"
      },
      "source": [
        "####3.1 Grid Search\n",
        "\n",
        "The first method is called Grid Search which is the most basic hyperparameter tuning method. Instead of that, it is also a good method to optimize the parameters when we have a small number of parameters (Thalles 2018). Grid Search will build and evaluate a model for each different parameter set in a grid. Therefore, the main problem for gird search is that it is an exponential time algorithm. Additionally, it’s not very effective in exploring the hyperparameter spaces (Thalles 2018).\n",
        "\n",
        "####3.2 Random Search\n",
        "\n",
        "Random Search is the second method that developed by James & Yoshua. It solves the drawback of Grid Search and can reduce unnecessary computation. This is a parameter tuning approach that will sample algorithm parameters from a random distribution for a fixed number of iterations. For example, I can identify a maximum number between the range from 1 to 50 by using random search algorithm, instead of defining the maximum number directly. The disadvantages of random search are that it doesn’t use existing information details from previous experiments and hard to predict the next following experiments. The difference between these two search algorithms can see from the below figure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPnUd8FxW4gL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "aa322996-d00b-47b1-b473-8062b9bc7646"
      },
      "source": [
        "display(HTML('''<img src='https://github.com//JennyZhan165/ML_A3_12840221/blob/master/Figure3.png?raw=true' width='600'>'''))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src='https://github.com//JennyZhan165/ML_A3_12840221/blob/master/Figure3.png?raw=true' width='600'>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6cTdKoPR9ud",
        "colab_type": "text"
      },
      "source": [
        "####3.3 Informed Search (e.g. Bayesian Optimization)\n",
        "\n",
        "By considering the drawbacks of previous two search algorithms, much better way is using Bayesian Optimization because it saves much more time. The main difference between Bayesian Optimization and gird and random search is that they use previous evaluation results to identify the next values to evaluate (Timo 2018). The idea for Bayesian optimization is to build a surrogate model that minimizes an objective function and add some extra overhead by repeatedly re-evaluating to find the best places (Prahbu 2018). The most popular surrogate model is Gaussian process (GP). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zes1y1ZIQejO",
        "colab_type": "text"
      },
      "source": [
        "###4. Conclusion\n",
        "\n",
        "The type of parameters can be divided into 2 parts in machine learning algorithm: Model Parameter and Hyperparameter. The reason why many different models will use the same way is to find the best combination of parameter is to improve performance before presenting results. There are many approaches to deal with the problem of learning. On the one hand, we can use hyperparameter tuning strategies, like gird search and random search. On the other hand, iterative procedures such as Bayesian Optimization, or more complex like iterated F-racing.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvnRj07GQlGr",
        "colab_type": "text"
      },
      "source": [
        "### 5. Reference\n",
        "\n",
        "\n",
        "* Aloïs, B.  ‘Hyper-parameter optimization algorithm: a short review’, Medium , weblog, viewed 8 October 2019,  <https://medium.com/criteo-labs/hyper-parameter-optimization-algorithms-2fe447525903>\n",
        "\n",
        "* Jason, B. 2014, ‘How to tune algorithm parameters with scikit-learn’, Machine Learning Mastery, viewed 2 October 2019, <https://machinelearningmastery.com/how-to-tune-algorithm-parameters-with-scikit-learn/> \n",
        "\n",
        "* Prahbu, 2018, ‘Understanding Hyperparameters and its Optimisation techniques’, Medium, weblog,  viewed 4 October 2019,  <https://towardsdatascience.com/understanding-hyperparameters-and-its-optimisation-techniques-f0debba07568>\n",
        "\n",
        "* Sayak, P. 2018, ‘Hyperparameter Optimization in Machine Learning Models’, DataCamp, viewed 1 October 2019,  <https://www.datacamp.com/community/tutorials/parameter-optimization-machine-learning-models>\n",
        "\n",
        "* Timo, B. 2018, ‘How to optimize Hyperparameters of machine learning methods’, Medium, weblog, viewed 2 October 2019,  <https://towardsdatascience.com/how-to-optimize-hyperparameters-of-machine-learning-models-98baec703593>\n",
        "\n",
        "* Thalles, S. 2018, ‘An introduction to high-dimensional hyper-parameter tuning’, FreeCodeCamp, viewed 3 October 2019,  <https://www.freecodecamp.org/news/an-introduction-to-high-dimensional-hyper-parameter-tuning-df5c0106e5a4/>"
      ]
    }
  ]
}